# -*- coding: utf-8 -*-
"""searchfts5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x8BCStwn6jwe56Tr0LCgzV7mI7KvnT77

SPELL CHECK
"""

from spellchecker import SpellChecker

checker = SpellChecker()

def corrected_spelling(input):
  search_query = input.split() # splits the input query into a list of words

  new_query = "" # instantiates empty string for the new query
  for word in search_query:
    new_query += " " + checker.correction(word) # adds all of the corrected words to the new_query string

  return new_query.strip() # removes excess whitespace and returns corrected query

"""LEMMATIZER"""

import nltk

from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

def pos_tagger(nltk_tag):
    if nltk_tag.startswith('J'):
        return wordnet.ADJ
    elif nltk_tag.startswith('V'):
        return wordnet.VERB
    elif nltk_tag.startswith('N'):
        return wordnet.NOUN
    elif nltk_tag.startswith('R'):
        return wordnet.ADV
    else:
        return None

def lemmatizer(searchQuery):
  lemmatizer = WordNetLemmatizer()
  #tags all the words in the search query
  pos_tagged = nltk.pos_tag(nltk.word_tokenize(searchQuery))
  #uses the pos_tag funtion to extract the wordnet tag for lemmatization
  wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))
  lemmatized_sentence = []
  for word, tag in wordnet_tagged:

      if (tag is None):
        if(word!='the'):
         # if there is no available tag, append the token as is
            lemmatized_sentence.append(word)
      elif(lemmatizer.lemmatize(word, tag)!="be"):
           # else use the tag to lemmatize the token(apart from verbs in "be" form)
          lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))
  #joins all the words in the list to form the updated search query
  lemmatized_sentence = " ".join(lemmatized_sentence)
  return lemmatized_sentence

"""FTS FUNCTION"""


import sqlite3
import pandas as pd
import csv

#function to crease fts5 table in a sample database
"""def createFTS5():
  df = pd.read_csv('content.csv')
  db = sqlite3.connect('solarspell2.db')
  cur = db.cursor()
  cur.execute('create virtual table content2 using fts5(id,title,	description,	file_name,	published_date,	copyright_notes,	rights_statement,	file_size,	keywords, tokenize="porter unicode61");')
  cur.executemany('insert into content2 (id,title,	description,	file_name,	published_date,	copyright_notes,	rights_statement,	file_size,	keywords) values (?,?,?,?,?,?,?,?,?);', df[['id','title','description','file_name','published_date','copyright_notes', 'rights_statement',	'file_size',	'keywords']].to_records(index=False))
  db.commit()
  db.close()"""

def searchQuery(searchString):
  conn = sqlite3.connect('solarspell2.db')
  cur = conn.cursor()
  data = cur.execute('SELECT rowid as id,highlight(content2, 2 , \'<mark>\', \'</mark>\') as title,file_name, description, file_size FROM content2 where content2 match "{}" order by rank limit 10'.format(searchString)).fetchall()
  return data

def completeSearch(searchString):
  search_string = corrected_spelling(searchString.lower())
  print("post spell-check: "+ search_string+"\n")
  search_string = lemmatizer(search_string)
  print("post lemmatization: "+search_string+"\n")
  result = searchQuery(search_string)
  jsonResult = json.dumps({"searchString" : searchString,"contentList" :convertJson(result) })
  return jsonResult

import json
# searchQuery method should return a json file

def convertJson(listOfResult):
  keys = ["id", "title", "file_name","description", "file_size"]
  listOfDicts = []
  #print("Length of list = "+str(len(listOfResult)))
  for j in range(0,len(listOfResult)):
    jDict = {}
    for i in range (0,5):
      #print("item in tuple"+str(listOfResult[j][i]))
      jDict.update({keys[i]:listOfResult[j][i]})
    listOfDicts.append(jDict)
  return json.dumps(listOfDicts)




